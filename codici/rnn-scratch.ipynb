{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Implementation of Recurrent Neural Networks from Scratch\n",
    "\n",
    "Implementation of an RNN from scratch for a character-level language model.\n",
    "The model will be trained on H. G. Wells' *The Time Machine*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l.torch import load_data_time_machine, Accumulator, Animator, Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    \"\"\"Accumulates values in a dictionary of sequences.\"\"\"\n",
    "    def __init__(self, keys):\n",
    "        self.data = {}\n",
    "        self.keys = keys\n",
    "        for k in self.keys:\n",
    "            self.data[k] = []\n",
    "\n",
    "    def add(self, *args):\n",
    "        for k,a in zip(self.keys, args):\n",
    "            self.data[k].append(a)\n",
    "\n",
    "    def sums(self):\n",
    "        return {k: sum(self.data[k]) for k in self.keys}\n",
    "\n",
    "    def merge(self, d):\n",
    "        for k in self.keys:\n",
    "            self.data[k].extend(d[k])\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "n_tokens = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the _batch_size_ items in a batch is a sequence of _num_steps_ tokens (characters), each identified by a token_id integer. The corresponding target is the sequence of _num_steps_ tokens derived from shifting the sequence one token right in the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_item = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  1,  3,  ..., 21,  2,  1],\n",
       "         [ 1,  4,  6,  ...,  4,  5, 10],\n",
       "         [ 4,  3,  1,  ...,  6,  2,  8],\n",
       "         ...,\n",
       "         [ 1, 15,  7,  ...,  1, 21, 14],\n",
       "         [15, 10, 19,  ..., 13, 14,  8],\n",
       "         [ 2,  1, 13,  ...,  5, 10,  1]]),\n",
       " tensor([[ 1,  3,  5,  ...,  2,  1, 15],\n",
       "         [ 4,  6, 11,  ...,  5, 10,  8],\n",
       "         [ 3,  1,  4,  ...,  2,  8,  8],\n",
       "         ...,\n",
       "         [15,  7,  6,  ..., 21, 14,  3],\n",
       "         [10, 19,  8,  ..., 14,  8,  3],\n",
       "         [ 1, 13,  2,  ..., 10,  1,  4]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  1,  3,  5, 13,  2,  1,  3, 10,  4, 22,  2, 12, 12,  2, 10,  1, 16,\n",
       "         7, 10,  1,  8,  7,  1,  5,  3,  1, 17,  5, 12, 12,  1, 21,  2,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_item[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  5, 13,  2,  1,  3, 10,  4, 22,  2, 12, 12,  2, 10,  1, 16,  7,\n",
       "        10,  1,  8,  7,  1,  5,  3,  1, 17,  5, 12, 12,  1, 21,  2,  1, 15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_item[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary associating the number of occurrences to each token is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 29927),\n",
       " ('e', 17838),\n",
       " ('t', 13515),\n",
       " ('a', 11704),\n",
       " ('i', 10138),\n",
       " ('n', 9917),\n",
       " ('o', 9758),\n",
       " ('s', 8486),\n",
       " ('h', 8257),\n",
       " ('r', 7674),\n",
       " ('d', 6337),\n",
       " ('l', 6146),\n",
       " ('m', 4043),\n",
       " ('u', 3805),\n",
       " ('c', 3424),\n",
       " ('f', 3354),\n",
       " ('w', 3225),\n",
       " ('g', 3075),\n",
       " ('y', 2679),\n",
       " ('p', 2427),\n",
       " ('b', 1897),\n",
       " ('v', 1295),\n",
       " ('k', 1087),\n",
       " ('x', 236),\n",
       " ('z', 144),\n",
       " ('j', 97),\n",
       " ('q', 95)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "### One-Hot Encoding\n",
    "Each token is encoded as a binary vector of length equal to the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(batch_item[0][0,0], n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "The shape of the minibatch that we sample each time is (_batch_size_, _num_steps_).\n",
    "The `one_hot` function transforms such a minibatch into a three-dimensional tensor with shape (_batch_size_, _num_steps_, _n_tokens_).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(batch_item[0], n_tokens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transpose the input so that we will obtain an output of shape (_num_steps_, _batch_size_, _n_tokens_)\n",
    "\n",
    "This will allow to more conveniently loop through the outermost dimension for updating hidden states of a minibatch, time step by time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 32, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(batch_item[0].T, n_tokens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## Initializing the Model Parameters\n",
    "\n",
    "The number of hidden units _num_hiddens_ of the RNN is a tunable hyperparameter.\n",
    "When training language models, the inputs and outputs are from the same vocabulary.\n",
    "Hence, they have the same dimension, which is equal to the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def init_params(n_tokens, num_hiddens):\n",
    "    num_inputs = num_outputs = n_tokens\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape) * 0.01\n",
    "\n",
    "    # Hidden layer parameters\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens)\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs)\n",
    "    # Attach gradients\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## RNN Model\n",
    "\n",
    "To define an RNN model, we first need a function to return the hidden state at initialization.\n",
    "It returns a tensor filled with 0 and with shape (_batch_size_, _num_hiddens_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def init_state(batch_size, num_hiddens):\n",
    "    return torch.zeros(batch_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "The RNN model loops through the outermost dimension of `inputs` so that it updates hidden states of a minibatch, time step by time step.\n",
    "Besides, the activation function here uses the $\\tanh$ function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "Let us check whether the outputs have the correct shapes, e.g., to ensure that the dimensionality of the hidden state remains unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 512\n",
    "\n",
    "#device = try_gpu()\n",
    "\n",
    "W_xh, W_hh, b_h, W_hq, b_q = init_params(n_tokens, num_hiddens)\n",
    "state = init_state(batch_size, num_hiddens) # shape: (batch_size, num_hiddens)\n",
    "\n",
    "X0 = batch_item[0]  # shape: (batch_size, num_steps)\n",
    "X=F.one_hot(X0.T, n_tokens).type(torch.float32) # shape: (num_steps, batch_size, n_tokens)\n",
    "H = state\n",
    "outputs = []\n",
    "for x in X: # for each time step\n",
    "    H = torch.tanh(torch.mm(x, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "    Y = torch.tanh(torch.mm(H, W_hq) + b_q)\n",
    "    outputs.append(Y)\n",
    "preds = torch.cat(outputs, dim=0) # shape: (num_steps x batch_size, n_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([1120, 28])\n"
     ]
    }
   ],
   "source": [
    "print(H.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now encapsulate everything in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch: #@save\n",
    "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, init_params):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = init_params(vocab_size, num_hiddens)\n",
    "        #self.init_state = init_state\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self._forward(X, state, self.params)\n",
    "\n",
    "    def _forward(self, inputs, state, params):\n",
    "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "        H = state\n",
    "        outputs = []\n",
    "        # Shape of `X`: (`batch_size`, `vocab_size`)\n",
    "        for X in inputs:\n",
    "            H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "            Y = torch.mm(H, W_hq) + b_q\n",
    "            outputs.append(Y)\n",
    "        return torch.cat(outputs, dim=0), H\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "torch.Size([32, 512])\n",
      "torch.Size([1120, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(n_tokens, num_hiddens, init_params)\n",
    "print(batch_size)\n",
    "state = net.init_state(X0.shape[0])\n",
    "preds, new_state = net(X0, state)\n",
    "\n",
    "print(new_state.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "We can see that the output shape is (number of time steps $\\times$ batch size, vocabulary size), while the hidden state shape remains the same, i.e., (batch size, number of hidden units).\n",
    "\n",
    "\n",
    "## Prediction\n",
    "\n",
    "Let us first define the prediction function\n",
    "to generate new characters following\n",
    "a user-provided _prefix_,\n",
    "which is a string containing several characters.\n",
    "When looping through these beginning characters in _prefix_,\n",
    "we keep passing the hidden state\n",
    "to the next time step without\n",
    "generating any output.\n",
    "This is called the *warm-up* period,\n",
    "during which the model updates itself\n",
    "(e.g., update the hidden state)\n",
    "but does not make predictions.\n",
    "After the warm-up period,\n",
    "the hidden state is generally better than\n",
    "its initialized value at the beginning.\n",
    "So we generate the predicted characters and emit them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'time traveller '\n",
    "prefix = 'in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sit down on or to eat it was a hobbit hole and that means comfort '#\n",
    "state = net.init_state(1)\n",
    "outputs = [vocab[prefix[0]]]\n",
    "get_input = lambda: torch.tensor([outputs[-1]]).reshape((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prefix[1]\n",
    "z = get_input()\n",
    "_, state = net(get_input(), state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_seq(prefix, num_preds, net, vocab):\n",
    "    \"\"\"Generate new characters following the prefix.\"\"\"\n",
    "    state = net.init_state(batch_size=1,)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]]).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The model training procedure which is introduced here differs from how we train a MLP in three aspects:\n",
    "\n",
    "### Sampling method\n",
    "Different sampling methods for sequential data can be applied and  result in differences in the initialization of hidden states.\n",
    "\n",
    "#### Random sampling\n",
    "In random sampling, each example is a subsequence arbitrarily captured on the original long sequence.\n",
    "The subsequences from two adjacent random minibatches during iteration are not necessarily adjacent on the original sequence.\n",
    "The following code randomly generates a minibatch from the data each time.\n",
    "Here, the argument _batch_size_ specifies the number of subsequence examples in each minibatch\n",
    "and _num_steps_ is the predefined number of time steps\n",
    "in each subsequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(seq, batch_size, num_steps):  #@save\n",
    "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
    "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
    "    # sequence\n",
    "    seq = seq[random.randint(0, num_steps - 1):]\n",
    "    # Subtract 1 since we need to account for labels\n",
    "    num_subseqs = (len(seq) - 1) // num_steps\n",
    "    # The starting indices for subsequences of length `num_steps`\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # In random sampling, the subsequences from two adjacent random\n",
    "    # minibatches during iteration are not necessarily adjacent on the\n",
    "    # original sequence\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # Return a sequence of length `num_steps` starting from `pos`\n",
    "        return seq[pos:pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Here, `initial_indices` contains randomized starting indices for\n",
    "        # subsequences\n",
    "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us manually generate a sequence from 0 to 34.\n",
    "We assume that\n",
    "the batch size and numbers of time steps are 2 and 5,\n",
    "respectively.\n",
    "This means that we can generate $\\lfloor (35 - 1) / 5 \\rfloor= 6$ feature-label subsequence pairs. With a minibatch size of 2, we only get 3 minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[16, 17, 18, 19, 20],\n",
      "        [21, 22, 23, 24, 25]]) \n",
      "Y: tensor([[17, 18, 19, 20, 21],\n",
      "        [22, 23, 24, 25, 26]])\n",
      "X:  tensor([[ 1,  2,  3,  4,  5],\n",
      "        [26, 27, 28, 29, 30]]) \n",
      "Y: tensor([[ 2,  3,  4,  5,  6],\n",
      "        [27, 28, 29, 30, 31]])\n",
      "X:  tensor([[11, 12, 13, 14, 15],\n",
      "        [ 6,  7,  8,  9, 10]]) \n",
      "Y: tensor([[12, 13, 14, 15, 16],\n",
      "        [ 7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential partitioning\n",
    "In addition to random sampling of the original sequence, we can also ensure that \n",
    "the subsequences from two adjacent minibatches\n",
    "during iteration\n",
    "are adjacent on the original sequence.\n",
    "This strategy preserves the order of split subsequences when iterating over minibatches, hence is called sequential partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
    "    # Start with a random offset to partition a sequence\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i:i + num_steps]\n",
    "        Y = Ys[:, i:i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same settings,\n",
    "let us print features and labels for each minibatch of subsequences read by sequential partitioning.\n",
    "Note that\n",
    "the subsequences from two adjacent minibatches\n",
    "during iteration\n",
    "are indeed adjacent on the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0,  1,  2,  3,  4],\n",
      "        [17, 18, 19, 20, 21]]) \n",
      "Y: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [18, 19, 20, 21, 22]])\n",
      "X:  tensor([[ 5,  6,  7,  8,  9],\n",
      "        [22, 23, 24, 25, 26]]) \n",
      "Y: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [23, 24, 25, 26, 27]])\n",
      "X:  tensor([[10, 11, 12, 13, 14],\n",
      "        [27, 28, 29, 30, 31]]) \n",
      "Y: tensor([[11, 12, 13, 14, 15],\n",
      "        [28, 29, 30, 31, 32]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 41
   },
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "For a sequence of length $T$, we compute the gradients over these $T$ time steps in an iteration, which results in a chain of matrix-products with length  $\\mathcal{O}(T)$ during backpropagation.\n",
    "This might result in numerical instability, e.g., the gradients may either explode or vanish, when $T$ is large. Therefore, RNN models often need extra help to stabilize the training.\n",
    "\n",
    "Generally speaking, when solving an optimization problem, we take update steps for the model parameter,\n",
    "say in the vector form $\\mathbf{x}$, in the direction of the negative gradient $\\mathbf{g}$ on a minibatch.\n",
    "For example, with $\\eta > 0$ as the learning rate, in one iteration we update $\\mathbf{x}$\n",
    "as $\\mathbf{x} - \\eta \\mathbf{g}$.\n",
    "\n",
    "Sometimes the gradients can be quite large and the optimization algorithm may fail to converge. We could address this by reducing the learning rate $\\eta$. But what if we only *rarely* get large gradients? In this case such an approach may appear entirely unwarranted. One popular alternative is to clip the gradient $\\mathbf{g}$ by projecting them back to a ball of a given radius, say $\\theta$ via\n",
    "\n",
    "$$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}$$\n",
    "\n",
    "By doing so we know that the gradient norm never exceeds $\\theta$ and that the\n",
    "updated gradient is entirely aligned with the original direction of $\\mathbf{g}$.\n",
    "\n",
    "Below we define a function to clip the gradients of\n",
    "a model. Note that we compute the gradient norm over all the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in net.params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "We use perplexity to evaluate the model. Perplexity of a sequence $x_1,\\ldots,x_n$ wrt a language model $P$ is strongly related to the cross-entropy loss averaged over all the $n$ tokens of the sequence:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1)$$\n",
    "\n",
    "This makes the performance on documents of different lengths comparable. Perplexity is indeed the exponential of such quantity:\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right)$$\n",
    "\n",
    "Let us look at a number of cases:\n",
    "\n",
    "* In the best case scenario, the model always perfectly estimates the probability of the label token as 1. In this case the perplexity of the model is 1.\n",
    "* In the worst case scenario, the model always predicts the probability of the label token as 0. In this situation, the perplexity is $+\\infty$.\n",
    "* At the baseline, the model predicts a uniform distribution over all the available tokens of the vocabulary. In this case, the perplexity equals the number of unique tokens of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "state = None\n",
    "X,Y = batch_item\n",
    "state = net.init_state(batch_size=X.shape[0])\n",
    "y = Y.T.reshape(-1)\n",
    "y_hat, state = net(X, state)\n",
    "l = loss(y_hat, y.long()).mean()\n",
    "l.backward()\n",
    "grad_clipping(net, 1)\n",
    "updater(batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1120, 28])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_hiddens = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(cum_loss, n):\n",
    "    math.exp(cum_loss, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, use_random_iter):\n",
    "    \"\"\"Train a net within one epoch.\"\"\"\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    h_epoch = History(['loss', 'n_tokens'])\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # Initialize `state` when either it is the first iteration or\n",
    "            # using random sampling\n",
    "            state = net.init_state(batch_size=X.shape[0])\n",
    "        else:\n",
    "            state.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "        h_epoch.add(float(l)*len(y), len(y))\n",
    "    s = h_epoch.sums()\n",
    "    #return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()#math.exp(s['loss'] / s['n_tokens']), s['n_tokens'] / timer.stop()\n",
    "    return math.exp(s['loss'] / s['n_tokens']), s['n_tokens'] / timer.stop(), h_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, train_iter, vocab, lr, num_epochs, use_random_iter=False, prefix = 'time traveler'):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    h_batch = History(['loss', 'n_tokens'])\n",
    "    h_train = History(['perplexity', 'speed', 'prediction'])\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs], figsize=(8, 6))\n",
    "    # Initialize\n",
    "    updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_seq(prefix, 50, net, vocab)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed, h_epoch = train_epoch(net, train_iter, loss, updater, use_random_iter)\n",
    "        pred = predict(prefix)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(pred)\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "        h_train.add(ppl, speed, pred)\n",
    "        h_batch.merge(h_epoch.data)\n",
    "    #print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec')\n",
    "    #print(predict('time traveller'))\n",
    "    #print(predict('traveller'))\n",
    "    return h_train, h_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying sequential partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"503.746875pt\" height=\"370.91625pt\" viewBox=\"0 0 503.746875 370.91625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-06-22T15:36:39.566839</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 370.91625 \nL 503.746875 370.91625 \nL 503.746875 0 \nL 0 0 \nL 0 370.91625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 333.36 \nL 487.003125 333.36 \nL 487.003125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 40.603125 333.36 \nL 40.603125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m7c67e31540\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"40.603125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 10 -->\n      <g transform=\"translate(34.240625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 90.203125 333.36 \nL 90.203125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"90.203125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(83.840625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 139.803125 333.36 \nL 139.803125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"139.803125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 30 -->\n      <g transform=\"translate(133.440625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 189.403125 333.36 \nL 189.403125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"189.403125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 40 -->\n      <g transform=\"translate(183.040625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 239.003125 333.36 \nL 239.003125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"239.003125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 50 -->\n      <g transform=\"translate(232.640625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 288.603125 333.36 \nL 288.603125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"288.603125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 60 -->\n      <g transform=\"translate(282.240625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path d=\"M 338.203125 333.36 \nL 338.203125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"338.203125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 70 -->\n      <g transform=\"translate(331.840625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path d=\"M 387.803125 333.36 \nL 387.803125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"387.803125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 80 -->\n      <g transform=\"translate(381.440625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path d=\"M 437.403125 333.36 \nL 437.403125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"437.403125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 90 -->\n      <g transform=\"translate(431.040625 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path d=\"M 487.003125 333.36 \nL 487.003125 7.2 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m7c67e31540\" x=\"487.003125\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 100 -->\n      <g transform=\"translate(477.459375 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- epoch -->\n     <g transform=\"translate(248.575 361.636563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path d=\"M 40.603125 303.812468 \nL 487.003125 303.812468 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path id=\"m455a008f96\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m455a008f96\" x=\"40.603125\" y=\"303.812468\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 7 -->\n      <g transform=\"translate(27.240625 307.611687)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path d=\"M 40.603125 241.499625 \nL 487.003125 241.499625 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m455a008f96\" x=\"40.603125\" y=\"241.499625\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 8 -->\n      <g transform=\"translate(27.240625 245.298844)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path d=\"M 40.603125 179.186783 \nL 487.003125 179.186783 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m455a008f96\" x=\"40.603125\" y=\"179.186783\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 9 -->\n      <g transform=\"translate(27.240625 182.986001)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path d=\"M 40.603125 116.87394 \nL 487.003125 116.87394 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m455a008f96\" x=\"40.603125\" y=\"116.87394\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 120.673159)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path d=\"M 40.603125 54.561097 \nL 487.003125 54.561097 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m455a008f96\" x=\"40.603125\" y=\"54.561097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 11 -->\n      <g transform=\"translate(20.878125 58.360316)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- perplexity -->\n     <g transform=\"translate(14.798438 195.406563)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"166.113281\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"229.589844\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"257.373047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"317.146484\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"376.326172\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"404.109375\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"443.318359\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 40.603125 22.025455 \nL 90.203125 132.966304 \nL 139.803125 179.49454 \nL 189.403125 216.599728 \nL 239.003125 228.657487 \nL 288.603125 241.231806 \nL 338.203125 259.936673 \nL 387.803125 280.681141 \nL 437.403125 301.862325 \nL 487.003125 318.534545 \n\" clip-path=\"url(#p1505b3e15e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 333.36 \nL 40.603125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 487.003125 333.36 \nL 487.003125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 333.36 \nL 487.003125 333.36 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 487.003125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 424.728125 29.878125 \nL 480.003125 29.878125 \nQ 482.003125 29.878125 482.003125 27.878125 \nL 482.003125 14.2 \nQ 482.003125 12.2 480.003125 12.2 \nL 424.728125 12.2 \nQ 422.728125 12.2 422.728125 14.2 \nL 422.728125 27.878125 \nQ 422.728125 29.878125 424.728125 29.878125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 426.728125 20.298438 \nL 436.728125 20.298438 \nL 446.728125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- train -->\n     <g transform=\"translate(454.728125 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1505b3e15e\">\n   <rect x=\"40.603125\" y=\"7.2\" width=\"446.4\" height=\"326.16\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs, lr = 100, 1.5\n",
    "net = RNNModelScratch(n_tokens, num_hiddens, init_params)\n",
    "h_train_s, h_batch_s = train(net, train_iter, vocab, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdbc27807f0>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"716.45pt\" height=\"357.238125pt\" viewBox=\"0 0 716.45 357.238125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-06-22T15:44:01.889820</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 357.238125 \nL 716.45 357.238125 \nL 716.45 0 \nL 0 0 \nL 0 357.238125 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.65 333.36 \nL 709.25 333.36 \nL 709.25 7.2 \nL 39.65 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m368a8088ac\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"70.086364\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(66.905114 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"146.272505\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(136.728755 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"222.458647\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(212.914897 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"298.644789\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(289.101039 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"374.830931\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(365.287181 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"451.017072\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(441.473322 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"527.203214\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(517.659464 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"603.389356\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(593.845606 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m368a8088ac\" x=\"679.575498\" y=\"333.36\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(670.031748 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m46d7dc1065\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"322.587038\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2000 -->\n      <g transform=\"translate(7.2 326.386256)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"279.207984\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2250 -->\n      <g transform=\"translate(7.2 283.007203)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"235.828931\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2500 -->\n      <g transform=\"translate(7.2 239.62815)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"192.449878\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2750 -->\n      <g transform=\"translate(7.2 196.249096)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"149.070824\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 3000 -->\n      <g transform=\"translate(7.2 152.870043)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"105.691771\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 3250 -->\n      <g transform=\"translate(7.2 109.49099)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"62.312718\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 3500 -->\n      <g transform=\"translate(7.2 66.111936)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m46d7dc1065\" x=\"39.65\" y=\"18.933664\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 3750 -->\n      <g transform=\"translate(7.2 22.732883)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 70.086364 22.025455 \nL 71.610086 43.360468 \nL 74.657532 77.622943 \nL 75.419394 83.855816 \nL 76.181255 88.270086 \nL 77.704978 102.983393 \nL 78.466839 104.399911 \nL 79.228701 113.408041 \nL 79.990562 109.650785 \nL 80.752423 95.197874 \nL 81.514285 107.167014 \nL 82.276146 109.156125 \nL 83.038008 115.027177 \nL 83.799869 115.878098 \nL 84.561731 107.656067 \nL 85.323592 102.429426 \nL 86.085453 112.888268 \nL 87.609176 125.230812 \nL 88.371038 114.721281 \nL 89.132899 117.318014 \nL 89.89476 114.598774 \nL 90.656622 105.366388 \nL 91.418483 123.924337 \nL 92.180345 122.290375 \nL 92.942206 127.96664 \nL 93.704068 127.092691 \nL 94.465929 110.876268 \nL 95.22779 123.938006 \nL 95.989652 127.172014 \nL 96.751513 127.146392 \nL 97.513375 136.679994 \nL 98.275236 135.321903 \nL 99.037098 140.024092 \nL 99.798959 134.699176 \nL 100.56082 113.84724 \nL 101.322682 137.487963 \nL 102.084543 140.990337 \nL 102.846405 139.776298 \nL 103.608266 149.816422 \nL 105.131989 157.041024 \nL 105.89385 165.412573 \nL 106.655712 148.33638 \nL 107.417573 163.022397 \nL 108.179435 157.562372 \nL 108.941296 155.699057 \nL 109.703157 158.537467 \nL 110.465019 157.69206 \nL 111.22688 178.506467 \nL 112.750603 164.251448 \nL 113.512464 173.832449 \nL 114.274326 177.202401 \nL 115.036187 167.3958 \nL 115.798049 182.011019 \nL 116.55991 169.681865 \nL 117.321772 182.101509 \nL 118.083633 188.85156 \nL 118.845494 175.745851 \nL 119.607356 177.645306 \nL 120.369217 186.779094 \nL 121.131079 179.980578 \nL 121.89294 192.424454 \nL 122.654801 184.308991 \nL 123.416663 193.658972 \nL 124.178524 195.655821 \nL 124.940386 179.069979 \nL 125.702247 193.969872 \nL 126.464109 195.226446 \nL 127.22597 187.216439 \nL 127.987831 200.368018 \nL 128.749693 192.449011 \nL 129.511554 199.867844 \nL 130.273416 208.668862 \nL 131.035277 185.00474 \nL 131.797138 184.215582 \nL 133.320861 191.624454 \nL 134.082723 200.511282 \nL 134.844584 197.213425 \nL 135.606446 206.25575 \nL 136.368307 212.648057 \nL 137.130168 193.439813 \nL 137.89203 202.118417 \nL 138.653891 199.464462 \nL 139.415753 199.256979 \nL 140.177614 209.611755 \nL 140.939475 197.018128 \nL 141.701337 212.547513 \nL 142.463198 213.631817 \nL 143.22506 195.748072 \nL 143.986921 210.356572 \nL 144.748783 203.041989 \nL 145.510644 200.062678 \nL 146.272505 211.895781 \nL 147.034367 185.95185 \nL 147.796228 204.400404 \nL 148.55809 208.273959 \nL 149.319951 198.586065 \nL 150.081812 213.863533 \nL 150.843674 210.370287 \nL 151.605535 204.698424 \nL 152.367397 219.558794 \nL 153.129258 207.041803 \nL 153.89112 210.217663 \nL 154.652981 210.193013 \nL 155.414842 192.198113 \nL 156.176704 205.357105 \nL 156.938565 216.059617 \nL 157.700427 203.011872 \nL 158.462288 224.441963 \nL 159.22415 207.477388 \nL 159.986011 223.761411 \nL 160.747872 228.670619 \nL 161.509734 209.259433 \nL 162.271595 217.186224 \nL 163.033457 214.97383 \nL 163.795318 201.675512 \nL 164.557179 220.254219 \nL 165.319041 207.54434 \nL 166.080902 227.384207 \nL 166.842764 227.189605 \nL 167.604625 212.410828 \nL 168.366487 211.353954 \nL 169.128348 225.031468 \nL 169.890209 217.159629 \nL 170.652071 226.240596 \nL 171.413932 211.39292 \nL 172.175794 217.118345 \nL 172.937655 220.815182 \nL 173.699516 211.561112 \nL 174.461378 222.221924 \nL 175.223239 221.395282 \nL 175.985101 218.672474 \nL 176.746962 234.74276 \nL 177.508824 215.650582 \nL 178.270685 222.475601 \nL 179.032546 209.851487 \nL 179.794408 199.639928 \nL 180.556269 218.809298 \nL 181.318131 225.86784 \nL 182.079992 221.451346 \nL 182.841853 231.462279 \nL 183.603715 218.406565 \nL 184.365576 232.034641 \nL 185.127438 237.871545 \nL 185.889299 214.152146 \nL 186.651161 224.48774 \nL 187.413022 231.626949 \nL 188.174883 224.864666 \nL 188.936745 232.194446 \nL 189.698606 217.19753 \nL 190.460468 235.53998 \nL 191.222329 237.729068 \nL 191.98419 217.207306 \nL 192.746052 233.907917 \nL 193.507913 231.052688 \nL 194.269775 223.644465 \nL 195.031636 235.798708 \nL 195.793498 222.637445 \nL 196.555359 235.980476 \nL 197.31722 240.573548 \nL 198.079082 222.447106 \nL 198.840943 220.461794 \nL 199.602805 236.860726 \nL 200.364666 230.713432 \nL 201.126527 237.592337 \nL 201.888389 221.719572 \nL 202.65025 233.011867 \nL 203.412112 238.397295 \nL 204.173973 214.562618 \nL 204.935835 225.39213 \nL 205.697696 233.732822 \nL 206.459557 224.996347 \nL 207.221419 241.903931 \nL 207.98328 231.738846 \nL 208.745142 238.421898 \nL 209.507003 249.523807 \nL 210.268864 221.758585 \nL 211.030726 235.734628 \nL 211.792587 233.444579 \nL 212.554449 223.295895 \nL 213.31631 238.777464 \nL 214.078172 231.778461 \nL 214.840033 238.045018 \nL 215.601894 242.263991 \nL 216.363756 222.594679 \nL 217.125617 230.146862 \nL 217.887479 241.526728 \nL 218.64934 236.772321 \nL 219.411202 248.657087 \nL 220.173063 229.642888 \nL 220.934924 242.905298 \nL 221.696786 243.707893 \nL 222.458647 228.421065 \nL 223.220509 229.440919 \nL 223.98237 236.520775 \nL 224.744231 230.070318 \nL 225.506093 235.606979 \nL 226.267954 226.649955 \nL 227.029816 247.830908 \nL 227.791677 244.226831 \nL 228.553539 227.823776 \nL 230.077261 249.773315 \nL 230.839123 239.893877 \nL 231.600984 247.511853 \nL 232.362846 232.410315 \nL 233.124707 244.877451 \nL 233.886568 250.502146 \nL 234.64843 226.99315 \nL 235.410291 241.133075 \nL 236.172153 248.531754 \nL 236.934014 235.621667 \nL 237.695876 248.62558 \nL 238.457737 231.092072 \nL 239.219598 250.050021 \nL 239.98146 251.079234 \nL 240.743321 232.793774 \nL 241.505183 242.999263 \nL 242.267044 246.279605 \nL 243.028905 240.676595 \nL 243.790767 251.509443 \nL 244.552628 224.410131 \nL 245.31449 243.524411 \nL 246.076351 241.162173 \nL 246.838213 222.483293 \nL 247.600074 247.529831 \nL 248.361935 250.234568 \nL 249.123797 251.11871 \nL 250.64752 236.047243 \nL 251.409381 250.480091 \nL 252.171242 240.637952 \nL 252.933104 227.748345 \nL 253.694965 237.144567 \nL 254.456827 251.545167 \nL 255.218688 242.090749 \nL 255.98055 255.378967 \nL 256.742411 236.795488 \nL 257.504272 252.347854 \nL 258.266134 258.568124 \nL 259.027995 230.408138 \nL 259.789857 245.657296 \nL 260.551718 253.752279 \nL 261.313579 243.129322 \nL 262.075441 251.97028 \nL 262.837302 243.35506 \nL 264.361025 258.248745 \nL 265.122887 229.123718 \nL 265.884748 239.939423 \nL 266.646609 256.809663 \nL 267.408471 246.135739 \nL 268.170332 262.868645 \nL 268.932194 239.201094 \nL 269.694055 250.540927 \nL 270.455916 252.79062 \nL 271.217778 219.021414 \nL 271.979639 220.534168 \nL 272.741501 247.218468 \nL 273.503362 247.368497 \nL 274.265224 257.891465 \nL 275.027085 235.477337 \nL 275.788946 254.50335 \nL 276.550808 253.15119 \nL 277.312669 233.556012 \nL 278.074531 241.474556 \nL 278.836392 259.989553 \nL 279.598253 248.571508 \nL 280.360115 267.035446 \nL 281.121976 237.979873 \nL 281.883838 256.28294 \nL 282.645699 258.331451 \nL 283.407561 234.815643 \nL 284.169422 252.417957 \nL 284.931283 257.974263 \nL 285.693145 244.847288 \nL 286.455006 250.803918 \nL 287.216868 246.369539 \nL 288.740591 261.450552 \nL 289.502452 241.260076 \nL 290.264313 250.368658 \nL 291.026175 266.764764 \nL 291.788036 251.795833 \nL 292.549898 260.506824 \nL 293.311759 241.162683 \nL 294.07362 257.770904 \nL 294.835482 256.957514 \nL 295.597343 237.116442 \nL 296.359205 236.618307 \nL 297.121066 256.873233 \nL 297.882928 256.162101 \nL 298.644789 262.09626 \nL 299.40665 246.205888 \nL 300.168512 256.114702 \nL 300.930373 261.134555 \nL 301.692235 240.125871 \nL 302.454096 250.483196 \nL 303.215957 266.916229 \nL 303.977819 254.159738 \nL 304.73968 263.238667 \nL 305.501542 246.19676 \nL 306.263403 259.944146 \nL 307.025265 262.971923 \nL 307.787126 239.831095 \nL 308.548987 249.244739 \nL 309.310849 267.681015 \nL 310.07271 254.596342 \nL 310.834572 263.757142 \nL 311.596433 247.619348 \nL 312.358294 261.213322 \nL 313.120156 264.314631 \nL 313.882017 238.169286 \nL 315.40574 265.228334 \nL 316.167602 256.453912 \nL 316.929463 262.809847 \nL 317.691324 243.68644 \nL 318.453186 255.851108 \nL 319.215047 253.556611 \nL 319.976909 234.134026 \nL 320.73877 245.086556 \nL 321.500631 261.327674 \nL 322.262493 258.517342 \nL 323.024354 269.913101 \nL 323.786216 249.156565 \nL 324.548077 262.919936 \nL 325.309939 266.227523 \nL 326.0718 245.041195 \nL 326.833661 245.528534 \nL 327.595523 260.137266 \nL 328.357384 256.503767 \nL 329.119246 265.061115 \nL 329.881107 248.105807 \nL 330.642968 261.25493 \nL 331.40483 266.251663 \nL 332.166691 243.224723 \nL 332.928553 252.987446 \nL 333.690414 267.726283 \nL 334.452276 255.454491 \nL 335.214137 266.366339 \nL 335.975998 246.143152 \nL 336.73786 262.569977 \nL 337.499721 264.281085 \nL 338.261583 234.208763 \nL 339.023444 256.071796 \nL 339.785305 266.163072 \nL 340.547167 258.168727 \nL 341.309028 266.819345 \nL 342.07089 253.666468 \nL 342.832751 267.880436 \nL 343.594613 268.673717 \nL 344.356474 245.575238 \nL 345.118335 257.679395 \nL 345.880197 265.78851 \nL 346.642058 263.454351 \nL 347.40392 265.842118 \nL 348.165781 250.841727 \nL 348.927643 266.821059 \nL 349.689504 259.983576 \nL 350.451365 236.721261 \nL 351.213227 249.403108 \nL 351.975088 267.253771 \nL 352.73695 257.747598 \nL 353.498811 272.19291 \nL 354.260672 248.389787 \nL 355.022534 262.290816 \nL 355.784395 267.684629 \nL 356.546257 244.417495 \nL 357.308118 257.557027 \nL 358.06998 262.651246 \nL 358.831841 262.799051 \nL 359.593702 273.887802 \nL 360.355564 248.510811 \nL 361.117425 270.964369 \nL 361.879287 268.278629 \nL 362.641148 242.815503 \nL 363.403009 253.500454 \nL 364.164871 266.607321 \nL 364.926732 259.817052 \nL 365.688594 269.837391 \nL 366.450455 253.549105 \nL 367.212317 257.123204 \nL 367.974178 270.741318 \nL 368.736039 245.13377 \nL 369.497901 252.50108 \nL 370.259762 273.139372 \nL 371.021624 258.972294 \nL 371.783485 268.784918 \nL 372.545346 252.853866 \nL 373.307208 262.763328 \nL 374.069069 270.344144 \nL 374.830931 242.426994 \nL 375.592792 252.145468 \nL 376.354654 267.349913 \nL 377.116515 268.410402 \nL 377.878376 269.041191 \nL 378.640238 252.835564 \nL 380.163961 271.124313 \nL 380.925822 247.93326 \nL 381.687683 254.931382 \nL 382.449545 268.684698 \nL 383.211406 257.627084 \nL 383.973268 268.440287 \nL 384.735129 253.765762 \nL 385.496991 268.358184 \nL 386.258852 273.957349 \nL 387.020713 246.452153 \nL 387.782575 256.589716 \nL 388.544436 268.429909 \nL 389.306298 269.515974 \nL 390.068159 270.102282 \nL 390.83002 253.812605 \nL 391.591882 265.461671 \nL 392.353743 273.494937 \nL 393.115605 247.420947 \nL 393.877466 257.299133 \nL 394.639328 265.127372 \nL 395.401189 267.459771 \nL 396.16305 277.026733 \nL 396.924912 256.462669 \nL 397.686773 266.911503 \nL 398.448635 275.057129 \nL 399.210496 249.846522 \nL 399.972357 269.88215 \nL 400.734219 276.108814 \nL 401.49608 270.452797 \nL 402.257942 248.884354 \nL 403.019803 242.969239 \nL 403.781665 264.273718 \nL 404.543526 258.085743 \nL 405.305387 248.081899 \nL 406.067249 259.777483 \nL 406.82911 276.308698 \nL 407.590972 266.612881 \nL 408.352833 285.041188 \nL 409.114695 252.555569 \nL 409.876556 272.693269 \nL 410.638417 274.684049 \nL 411.400279 250.237256 \nL 412.16214 263.653354 \nL 412.924002 267.156887 \nL 413.685863 266.924523 \nL 414.447724 276.127764 \nL 415.209586 255.669804 \nL 415.971447 267.243438 \nL 416.733309 275.667948 \nL 417.49517 251.51088 \nL 418.257032 260.185406 \nL 419.018893 270.970207 \nL 419.780754 265.121395 \nL 420.542616 275.442533 \nL 421.304477 259.042583 \nL 422.066339 273.265446 \nL 422.8282 276.793582 \nL 423.590061 253.269388 \nL 424.351923 270.154593 \nL 425.113784 280.348545 \nL 425.875646 269.232272 \nL 426.637507 275.403613 \nL 427.399369 253.996087 \nL 428.16123 272.756608 \nL 428.923091 272.782694 \nL 429.684953 247.880393 \nL 430.446814 256.497836 \nL 431.208676 274.893478 \nL 431.970537 264.3801 \nL 432.732398 276.969928 \nL 433.49426 258.417725 \nL 435.017983 276.682149 \nL 435.779844 250.537684 \nL 437.303567 276.857198 \nL 438.065428 267.549844 \nL 438.82729 282.151486 \nL 439.589151 256.835193 \nL 440.351013 273.073856 \nL 441.112874 277.225367 \nL 441.874735 244.872493 \nL 443.398458 270.228681 \nL 444.922181 276.478697 \nL 445.684043 260.667742 \nL 446.445904 271.516854 \nL 447.207765 278.200091 \nL 447.969627 256.629563 \nL 448.731488 263.506383 \nL 449.49335 275.107725 \nL 450.255211 275.736105 \nL 451.017072 279.988067 \nL 451.778934 258.139444 \nL 452.540795 273.987003 \nL 453.302657 270.931657 \nL 454.064518 244.821943 \nL 454.82638 265.780494 \nL 455.588241 276.878743 \nL 456.350102 278.858356 \nL 457.111964 271.046426 \nL 457.873825 265.855323 \nL 458.635687 273.139186 \nL 459.397548 263.798055 \nL 460.159409 249.214251 \nL 460.921271 270.169327 \nL 461.683132 281.3153 \nL 462.444994 273.711363 \nL 463.206855 281.041467 \nL 463.968717 255.292137 \nL 464.730578 277.646031 \nL 465.492439 278.647722 \nL 466.254301 251.728324 \nL 467.016162 264.81397 \nL 467.778024 272.479717 \nL 468.539885 274.045569 \nL 469.301747 284.571989 \nL 470.063608 263.120932 \nL 470.825469 273.845592 \nL 471.587331 280.049228 \nL 472.349192 259.785082 \nL 473.111054 273.640426 \nL 473.872915 278.039313 \nL 474.634776 277.807041 \nL 475.396638 273.996733 \nL 476.158499 259.880391 \nL 476.920361 277.151881 \nL 477.682222 271.346994 \nL 478.444084 253.942525 \nL 479.205945 276.606671 \nL 479.967806 277.921625 \nL 480.729668 278.28618 \nL 481.491529 284.278765 \nL 482.253391 262.532029 \nL 483.015252 275.016031 \nL 483.777113 265.994 \nL 484.538975 254.068275 \nL 485.300836 274.548754 \nL 486.062698 281.843181 \nL 486.824559 281.856178 \nL 487.586421 280.390013 \nL 488.348282 265.782718 \nL 489.110143 279.755008 \nL 489.872005 275.040356 \nL 490.633866 253.998358 \nL 491.395728 270.950562 \nL 492.157589 280.22437 \nL 492.91945 273.728414 \nL 493.681312 282.339162 \nL 494.443173 265.030998 \nL 495.966896 282.557996 \nL 496.728758 255.337035 \nL 497.490619 260.066189 \nL 498.25248 271.95077 \nL 499.014342 273.49971 \nL 499.776203 283.641026 \nL 500.538065 264.497464 \nL 501.299926 278.496998 \nL 502.061787 283.880248 \nL 502.823649 259.241725 \nL 503.58551 278.584291 \nL 504.347372 280.811419 \nL 505.109233 281.374236 \nL 505.871095 289.079645 \nL 506.632956 264.287942 \nL 507.394817 277.301122 \nL 508.156679 268.531843 \nL 508.91854 255.432529 \nL 510.442263 277.853514 \nL 511.204124 279.875152 \nL 511.965986 288.519191 \nL 512.727847 266.019855 \nL 513.489709 281.890117 \nL 514.25157 281.344907 \nL 515.013432 257.128949 \nL 515.775293 281.458356 \nL 516.537154 286.73696 \nL 517.299016 281.07997 \nL 518.060877 284.748219 \nL 518.822739 271.610402 \nL 519.5846 283.695052 \nL 520.346461 284.547246 \nL 521.108323 260.477912 \nL 521.870184 272.453214 \nL 522.632046 280.396639 \nL 523.393907 279.675221 \nL 524.155769 289.004144 \nL 524.91763 261.509627 \nL 525.679491 280.090419 \nL 526.441353 281.837969 \nL 527.203214 255.538448 \nL 527.965076 272.00183 \nL 528.726937 282.179657 \nL 529.488798 281.39372 \nL 530.25066 292.013781 \nL 531.012521 268.197174 \nL 531.774383 269.416588 \nL 532.536244 279.753294 \nL 533.298106 255.117737 \nL 534.059967 276.362353 \nL 534.821828 282.97039 \nL 535.58369 275.744306 \nL 536.345551 292.321808 \nL 537.107413 265.232411 \nL 537.869274 286.895144 \nL 538.631136 285.143586 \nL 539.392997 261.026458 \nL 540.154858 274.944769 \nL 540.91672 283.928041 \nL 541.678581 279.900543 \nL 542.440443 296.134341 \nL 543.202304 272.759573 \nL 543.964165 279.941734 \nL 544.726027 291.140874 \nL 545.487888 266.834681 \nL 546.24975 280.11252 \nL 547.011611 285.364807 \nL 547.773473 278.583179 \nL 548.535334 290.11875 \nL 549.297195 270.337889 \nL 550.059057 286.126048 \nL 550.820918 285.207897 \nL 551.58278 263.787444 \nL 552.344641 275.162863 \nL 553.106502 292.799093 \nL 553.868364 282.735594 \nL 554.630225 299.101304 \nL 555.392087 273.232827 \nL 556.153948 281.472464 \nL 556.91581 283.012462 \nL 557.677671 253.638761 \nL 558.439532 276.160337 \nL 559.201394 284.90038 \nL 559.963255 285.708395 \nL 560.725117 293.406993 \nL 561.486978 276.356376 \nL 562.248839 289.826361 \nL 563.010701 284.313029 \nL 563.772562 263.641632 \nL 564.534424 282.065908 \nL 565.296285 293.887521 \nL 566.058147 284.376414 \nL 566.820008 297.74553 \nL 567.581869 269.138724 \nL 568.343731 283.218462 \nL 569.105592 288.962768 \nL 569.867454 264.497835 \nL 570.629315 274.074203 \nL 571.391176 285.386329 \nL 572.914899 294.948403 \nL 573.676761 271.655809 \nL 574.438622 285.562258 \nL 575.200484 285.294449 \nL 575.962345 271.235746 \nL 576.724206 284.730589 \nL 578.247929 296.366218 \nL 579.009791 281.646564 \nL 580.533513 282.68509 \nL 582.057236 255.960039 \nL 582.819098 289.385425 \nL 583.580959 289.488471 \nL 584.342821 290.426986 \nL 585.104682 299.891945 \nL 585.866543 277.632734 \nL 586.628405 292.826476 \nL 587.390266 283.221983 \nL 588.152128 258.516508 \nL 588.913989 274.809984 \nL 589.67585 285.213365 \nL 590.437712 281.159711 \nL 591.199573 298.3892 \nL 591.961435 274.230858 \nL 592.723296 278.840425 \nL 593.485158 290.753246 \nL 594.247019 264.79275 \nL 595.00888 287.674015 \nL 595.770742 299.108115 \nL 596.532603 287.801943 \nL 597.294465 305.099519 \nL 598.056326 276.562051 \nL 598.818188 294.139322 \nL 599.580049 296.415819 \nL 600.34191 272.916738 \nL 601.103772 281.679785 \nL 601.865633 280.623026 \nL 602.627495 285.354683 \nL 603.389356 287.222145 \nL 604.151217 281.343841 \nL 604.913079 291.381787 \nL 606.436802 270.80373 \nL 607.198663 290.790498 \nL 607.960525 301.856221 \nL 608.722386 290.349168 \nL 609.484247 300.292825 \nL 610.246109 281.713979 \nL 611.00797 294.235557 \nL 611.769832 296.160218 \nL 612.531693 267.650991 \nL 613.293554 290.530588 \nL 614.055416 303.980766 \nL 614.817277 288.589433 \nL 615.579139 311.416511 \nL 616.341 281.342405 \nL 617.102862 292.148705 \nL 617.864723 289.044176 \nL 618.626584 252.7517 \nL 619.388446 271.33263 \nL 620.150307 280.509462 \nL 620.912169 286.13791 \nL 622.435891 275.768585 \nL 623.197753 296.119027 \nL 623.959614 294.02914 \nL 624.721476 277.00301 \nL 625.483337 291.195595 \nL 626.245199 301.527853 \nL 627.00706 299.646074 \nL 627.768921 293.475404 \nL 628.530783 286.346852 \nL 629.292644 296.73163 \nL 630.054506 287.453489 \nL 630.816367 267.75274 \nL 631.578228 297.857472 \nL 632.34009 297.434259 \nL 633.101951 299.310409 \nL 633.863813 296.370249 \nL 634.625674 281.013551 \nL 635.387536 297.5001 \nL 636.149397 293.280432 \nL 636.911258 266.601576 \nL 637.67312 292.682656 \nL 638.434981 307.846304 \nL 639.196843 296.015494 \nL 639.958704 308.139273 \nL 640.720565 280.444873 \nL 641.482427 300.336749 \nL 642.244288 305.102786 \nL 643.00615 277.366314 \nL 643.768011 301.883836 \nL 644.529873 303.232475 \nL 645.291734 308.856544 \nL 646.815457 283.270611 \nL 647.577318 301.979098 \nL 648.33918 292.002892 \nL 649.101041 264.633222 \nL 649.862902 285.283329 \nL 650.624764 293.11634 \nL 651.386625 294.662153 \nL 652.148487 301.38306 \nL 652.910348 282.793117 \nL 653.67221 298.553221 \nL 654.434071 303.274592 \nL 655.195932 281.514443 \nL 655.957794 294.546643 \nL 656.719655 300.258723 \nL 657.481517 304.369182 \nL 658.243378 289.823696 \nL 659.00524 289.785958 \nL 659.767101 293.661597 \nL 660.528962 286.654787 \nL 661.290824 276.771017 \nL 662.052685 296.347963 \nL 662.814547 305.188944 \nL 663.576408 296.282308 \nL 664.338269 307.290322 \nL 665.100131 288.441535 \nL 665.861992 302.532996 \nL 666.623854 300.656128 \nL 667.385715 263.870799 \nL 668.147577 296.698131 \nL 668.909438 302.633656 \nL 669.671299 305.061526 \nL 670.433161 308.812828 \nL 671.195022 291.80778 \nL 671.956884 303.953406 \nL 672.718745 306.256521 \nL 673.480606 279.096465 \nL 674.242468 303.719814 \nL 675.004329 298.459094 \nL 675.766191 303.204141 \nL 676.528052 318.534545 \nL 677.289914 287.942265 \nL 678.051775 305.309643 \nL 678.813636 288.752644 \nL 678.813636 288.752644 \n\" clip-path=\"url(#p94af61901a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 39.65 333.36 \nL 39.65 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 709.25 333.36 \nL 709.25 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 39.65 333.36 \nL 709.25 333.36 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 39.65 7.2 \nL 709.25 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p94af61901a\">\n   <rect x=\"39.65\" y=\"7.2\" width=\"669.6\" height=\"326.16\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(h_batch_s.data['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 1.4, 12900.5 tokens/sec\n",
      "time traveller smiled frempect ces lers found mun alkays have do\n",
      "traveller smiled fremmedex an which from ane dradce is in f\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"180.65625pt\" viewBox=\"0 0 262.1875 180.65625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-06-22T15:23:26.419318</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 262.1875 180.65625 \nL 262.1875 0 \nL 0 0 \nL 0 180.65625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 143.1 \nL 245.44375 143.1 \nL 245.44375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 86.015179 143.1 \nL 86.015179 7.2 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m674d84b6ab\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m674d84b6ab\" x=\"86.015179\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 100 -->\n      <g transform=\"translate(76.471429 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 125.872321 143.1 \nL 125.872321 7.2 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m674d84b6ab\" x=\"125.872321\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(116.328571 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 165.729464 143.1 \nL 165.729464 7.2 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m674d84b6ab\" x=\"165.729464\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 300 -->\n      <g transform=\"translate(156.185714 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 205.586607 143.1 \nL 205.586607 7.2 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m674d84b6ab\" x=\"205.586607\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 400 -->\n      <g transform=\"translate(196.042857 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 245.44375 143.1 \nL 245.44375 7.2 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m674d84b6ab\" x=\"245.44375\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 500 -->\n      <g transform=\"translate(235.9 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <g transform=\"translate(132.565625 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 50.14375 125.461608 \nL 245.44375 125.461608 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"ma33a6f8886\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma33a6f8886\" x=\"50.14375\" y=\"125.461608\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2.5 -->\n      <g transform=\"translate(27.240625 129.260827)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 50.14375 100.701204 \nL 245.44375 100.701204 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#ma33a6f8886\" x=\"50.14375\" y=\"100.701204\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5.0 -->\n      <g transform=\"translate(27.240625 104.500423)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 50.14375 75.9408 \nL 245.44375 75.9408 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#ma33a6f8886\" x=\"50.14375\" y=\"75.9408\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 7.5 -->\n      <g transform=\"translate(27.240625 79.740018)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 50.14375 51.180395 \nL 245.44375 51.180395 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#ma33a6f8886\" x=\"50.14375\" y=\"51.180395\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10.0 -->\n      <g transform=\"translate(20.878125 54.979614)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 50.14375 26.419991 \nL 245.44375 26.419991 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#ma33a6f8886\" x=\"50.14375\" y=\"26.419991\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 12.5 -->\n      <g transform=\"translate(20.878125 30.21921)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- perplexity -->\n     <g transform=\"translate(14.798437 100.276562)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"166.113281\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"229.589844\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"257.373047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"317.146484\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"376.326172\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"404.109375\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"443.318359\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 50.14375 13.377273 \nL 54.129464 45.078248 \nL 58.115179 53.034041 \nL 62.100893 60.21118 \nL 66.086607 64.320461 \nL 70.072321 67.886559 \nL 74.058036 70.120763 \nL 78.04375 71.589688 \nL 82.029464 73.041188 \nL 86.015179 75.174728 \nL 90.000893 75.96721 \nL 93.986607 80.080748 \nL 97.972321 81.584973 \nL 101.958036 84.577519 \nL 105.94375 87.471536 \nL 109.929464 90.831279 \nL 113.915179 93.412293 \nL 117.900893 96.782115 \nL 121.886607 103.143534 \nL 125.872321 109.009148 \nL 129.858036 109.238286 \nL 133.84375 114.894263 \nL 137.829464 121.27487 \nL 141.815179 121.027325 \nL 145.800893 124.954066 \nL 149.786607 126.820782 \nL 153.772321 128.553753 \nL 157.758036 130.031592 \nL 161.74375 130.015324 \nL 165.729464 130.861578 \nL 169.715179 131.994985 \nL 173.700893 133.598538 \nL 177.686607 134.162082 \nL 181.672321 133.973751 \nL 185.658036 132.954617 \nL 189.64375 133.400503 \nL 193.629464 134.483667 \nL 197.615179 133.760275 \nL 201.600893 134.339478 \nL 205.586607 135.302467 \nL 209.572321 135.036269 \nL 213.558036 134.938115 \nL 217.54375 135.765107 \nL 221.529464 135.844815 \nL 225.515179 136.631169 \nL 229.500893 135.211363 \nL 233.486607 135.317579 \nL 237.472321 136.4537 \nL 241.458036 136.922727 \nL 245.44375 136.677007 \n\" clip-path=\"url(#pb79c2d6852)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 143.1 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 245.44375 143.1 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 143.1 \nL 245.44375 143.1 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 183.16875 29.878125 \nL 238.44375 29.878125 \nQ 240.44375 29.878125 240.44375 27.878125 \nL 240.44375 14.2 \nQ 240.44375 12.2 238.44375 12.2 \nL 183.16875 12.2 \nQ 181.16875 12.2 181.16875 14.2 \nL 181.16875 27.878125 \nQ 181.16875 29.878125 183.16875 29.878125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 185.16875 20.298437 \nL 195.16875 20.298437 \nL 205.16875 20.298437 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- train -->\n     <g transform=\"translate(213.16875 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb79c2d6852\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"135.9\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "net0 = RNNModelScratch(n_tokens, num_hiddens, init_params)\n",
    "h_train, h_batch = train(net0, train_iter, vocab, lr, num_epochs, use_random_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17268.124099397148,\n",
       " 18287.529149564114,\n",
       " 19218.388959374963,\n",
       " 18670.171335911386,\n",
       " 18859.555040335228,\n",
       " 17571.93828751405,\n",
       " 18328.020151526605,\n",
       " 17631.054932135317,\n",
       " 20402.21619497026,\n",
       " 18115.760423043735,\n",
       " 13244.569914945116,\n",
       " 20374.640804512015,\n",
       " 18861.571181784144,\n",
       " 14738.655914547795,\n",
       " 15598.589372477954,\n",
       " 16224.244495425122,\n",
       " 15623.71827475005,\n",
       " 13888.629148373173,\n",
       " 14696.568935816591,\n",
       " 14694.477215737781,\n",
       " 8309.636773378712,\n",
       " 13023.73038866417,\n",
       " 14206.622747478275,\n",
       " 13536.948054825034,\n",
       " 13859.863278426374,\n",
       " 14513.03932631566,\n",
       " 14301.700229894506,\n",
       " 11902.073542666794,\n",
       " 9788.292283726092,\n",
       " 9162.546635020364,\n",
       " 8753.975080927934,\n",
       " 11245.287555394638,\n",
       " 11598.665551890042,\n",
       " 13318.133935599011,\n",
       " 11808.648195833772,\n",
       " 13110.208690470958,\n",
       " 14116.03997764329,\n",
       " 12897.990031255717,\n",
       " 11280.385072092959,\n",
       " 12980.953526407688,\n",
       " 9197.582610308811,\n",
       " 13190.829639083166,\n",
       " 11171.235620524227,\n",
       " 11179.862911860138,\n",
       " 13030.377489854734,\n",
       " 12937.766340568496,\n",
       " 12871.065859945298,\n",
       " 12524.040622440185,\n",
       " 10476.844158849193,\n",
       " 10269.09470879201,\n",
       " 7817.10113214015,\n",
       " 13365.019200608844,\n",
       " 16871.780825115773,\n",
       " 6128.879269067534,\n",
       " 11963.160217878003,\n",
       " 13610.980005266061,\n",
       " 13805.111154375976,\n",
       " 11731.311728669647,\n",
       " 12819.210393732052,\n",
       " 14250.608551359272,\n",
       " 10693.385931985209,\n",
       " 13775.568764643956,\n",
       " 14284.896879981816,\n",
       " 14585.927789477442,\n",
       " 14763.721242320744,\n",
       " 14167.755738774094,\n",
       " 14936.89694123408,\n",
       " 14659.494896223736,\n",
       " 14300.279849070848,\n",
       " 14986.729961844238,\n",
       " 11088.86557736159,\n",
       " 7833.511067625146,\n",
       " 7433.752453292981,\n",
       " 12918.80230167414,\n",
       " 14472.948196415746,\n",
       " 10134.01850988446,\n",
       " 14463.028183323995,\n",
       " 15612.31431875889,\n",
       " 15292.218682070801,\n",
       " 16043.203623502392,\n",
       " 13928.532612240784,\n",
       " 15579.009352969511,\n",
       " 17108.02720097456,\n",
       " 15898.988523661514,\n",
       " 16366.582820929005,\n",
       " 16754.238758274365,\n",
       " 16845.334320659167,\n",
       " 16018.305853990792,\n",
       " 16883.01370687653,\n",
       " 15350.77911579656,\n",
       " 12644.918673185688,\n",
       " 14394.56798635194,\n",
       " 15285.252525581194,\n",
       " 14975.11470731335,\n",
       " 14642.91903273493,\n",
       " 14403.803549116554,\n",
       " 14759.100134273735,\n",
       " 16294.527009068423,\n",
       " 16309.830148629171,\n",
       " 16081.75890047752,\n",
       " 14560.655251426388,\n",
       " 15619.718178965459,\n",
       " 14956.442702533615,\n",
       " 14882.359446887813,\n",
       " 15399.41519782496,\n",
       " 15533.335609403255,\n",
       " 15712.76721158216,\n",
       " 16168.410386582027,\n",
       " 16061.284254361823,\n",
       " 16389.66624465158,\n",
       " 13372.176353061946,\n",
       " 14250.543706108087,\n",
       " 11346.78261142614,\n",
       " 11762.731888580553,\n",
       " 14597.41752748307,\n",
       " 15002.865897060943,\n",
       " 14435.733570363178,\n",
       " 14649.905795491313,\n",
       " 14542.591068802723,\n",
       " 12802.471529028298,\n",
       " 9451.075551589905,\n",
       " 10016.6354616583,\n",
       " 7742.860359302361,\n",
       " 6673.280083415638,\n",
       " 7177.635199839873,\n",
       " 7714.177123884496,\n",
       " 10697.677894860293,\n",
       " 15026.999859250445,\n",
       " 9173.42972266343,\n",
       " 10279.543860359343,\n",
       " 12016.810261414877,\n",
       " 13598.563843221771,\n",
       " 10267.114015043477,\n",
       " 11377.124706799823,\n",
       " 12827.633468137221,\n",
       " 9816.401391187146,\n",
       " 10520.085300643337,\n",
       " 9002.737365648132,\n",
       " 12596.951414824602,\n",
       " 15847.612056358364,\n",
       " 6053.355763571459,\n",
       " 7731.429582431219,\n",
       " 8213.386042191418,\n",
       " 8310.748529789333,\n",
       " 9625.640285882893,\n",
       " 9074.877370352475,\n",
       " 10622.696156565142,\n",
       " 12937.766340568496,\n",
       " 14468.384749065815,\n",
       " 15059.25727468094,\n",
       " 8698.886404438308,\n",
       " 7316.244203660482,\n",
       " 6556.777539896654,\n",
       " 14749.895536681477,\n",
       " 11251.913596621758,\n",
       " 8978.647431797597,\n",
       " 12731.757039044634,\n",
       " 13649.92644556387,\n",
       " 12019.308386178982,\n",
       " 9850.527411705314,\n",
       " 7865.050216367047,\n",
       " 6396.7554473154405,\n",
       " 11742.579707980343,\n",
       " 12424.12037136522,\n",
       " 15080.698060305996,\n",
       " 15474.81673642097,\n",
       " 14887.058075071156,\n",
       " 14555.094865180985,\n",
       " 15113.946436550199,\n",
       " 15104.002675084179,\n",
       " 14177.59026381412,\n",
       " 10875.647722553656,\n",
       " 10644.883785946451,\n",
       " 11382.896785095632,\n",
       " 11516.852492455093,\n",
       " 12699.31339635068,\n",
       " 7534.50764919366,\n",
       " 12650.166837889476,\n",
       " 15079.148992856646,\n",
       " 15065.831682334783,\n",
       " 10361.09155352836,\n",
       " 13774.049023068192,\n",
       " 13628.141804467654,\n",
       " 8711.311535678142,\n",
       " 14027.154740407248,\n",
       " 14439.244475728245,\n",
       " 11869.939325256422,\n",
       " 11509.62178478478,\n",
       " 11118.33559412193,\n",
       " 14949.606574475813,\n",
       " 5462.126305597111,\n",
       " 11031.751678644332,\n",
       " 14504.390896817684,\n",
       " 8530.80780682824,\n",
       " 12075.357500779994,\n",
       " 10679.36824546572,\n",
       " 12500.80293145957,\n",
       " 11912.2144422545,\n",
       " 11354.033067848728,\n",
       " 9161.581690178271,\n",
       " 9586.826497431292,\n",
       " 13237.814456957072,\n",
       " 9313.387921901389,\n",
       " 12578.985278814192,\n",
       " 13802.834554976514,\n",
       " 13900.326283111832,\n",
       " 12511.727809315238,\n",
       " 12534.170026091617,\n",
       " 14156.585540236181,\n",
       " 14460.590644953507,\n",
       " 9729.452518745156,\n",
       " 13979.354325137754,\n",
       " 14654.984512831752,\n",
       " 14540.19414801213,\n",
       " 13610.980005266061,\n",
       " 12877.447685425523,\n",
       " 12888.152648330843,\n",
       " 14028.688955957094,\n",
       " 14553.792792300861,\n",
       " 14338.658305334622,\n",
       " 10502.956246443675,\n",
       " 13819.309927400842,\n",
       " 14630.127260935136,\n",
       " 13537.894086527187,\n",
       " 13393.24041461672,\n",
       " 14277.310007993265,\n",
       " 14381.144056105688,\n",
       " 10465.80080677614,\n",
       " 10928.87972259155,\n",
       " 12483.67790678207,\n",
       " 7287.376435307771,\n",
       " 11404.863433449564,\n",
       " 9485.718110770053,\n",
       " 12747.446536212776,\n",
       " 13523.367095303467,\n",
       " 10136.560578961324,\n",
       " 11362.124051983861,\n",
       " 10693.325077850417,\n",
       " 9914.019282510382,\n",
       " 11976.897049768819,\n",
       " 8135.314616239689,\n",
       " 10951.778974405901,\n",
       " 13409.259087858367,\n",
       " 12574.077868283388,\n",
       " 12852.032997210106,\n",
       " 11549.155590302524,\n",
       " 12478.218907768804,\n",
       " 12759.954950840802,\n",
       " 8135.693267550933,\n",
       " 11773.164027170938,\n",
       " 5596.334426461912,\n",
       " 10134.338248460203,\n",
       " 12946.555605201502,\n",
       " 10442.074601958215,\n",
       " 11509.766309958817,\n",
       " 11257.48173177446,\n",
       " 10469.124491077933,\n",
       " 12602.497913330706,\n",
       " 14188.7293635359,\n",
       " 12028.880043325904,\n",
       " 5999.004848406098,\n",
       " 12155.086857479602,\n",
       " 13986.976530916827,\n",
       " 12935.495196813778,\n",
       " 12673.21146049789,\n",
       " 12595.980329591952,\n",
       " 14194.42074613601,\n",
       " 12353.832512231065,\n",
       " 13091.164784206845,\n",
       " 13284.858103467232,\n",
       " 9632.617554392904,\n",
       " 12200.918402363503,\n",
       " 12239.633979843878,\n",
       " 12826.552070267771,\n",
       " 13467.847267491177,\n",
       " 13531.14796856159,\n",
       " 12343.773432203612,\n",
       " 12584.006078207023,\n",
       " 12771.389775673812,\n",
       " 11739.362791984951,\n",
       " 7049.372070496414,\n",
       " 10886.671394338844,\n",
       " 12439.640934571087,\n",
       " 13459.010999361448,\n",
       " 13354.338161689951,\n",
       " 12851.22433668113,\n",
       " 12372.223799162937,\n",
       " 11298.14178894482,\n",
       " 11759.51496217526,\n",
       " 11787.147959727754,\n",
       " 8616.783504625155,\n",
       " 11835.025562117804,\n",
       " 8953.329249881297,\n",
       " 10753.640875377712,\n",
       " 12361.199538456884,\n",
       " 11761.07903724361,\n",
       " 10327.456573169582,\n",
       " 10969.165363661705,\n",
       " 12143.009786536662,\n",
       " 11815.995844699413,\n",
       " 7887.248367446522,\n",
       " 10889.696646563647,\n",
       " 11813.410687498977,\n",
       " 10397.687179853241,\n",
       " 10836.032326186772,\n",
       " 7518.764185351391,\n",
       " 12764.314852723352,\n",
       " 12887.007993306335,\n",
       " 9903.649772917612,\n",
       " 10290.405945829165,\n",
       " 7908.808149188537,\n",
       " 10751.607289734653,\n",
       " 9849.36824396663,\n",
       " 12129.547161200388,\n",
       " 11737.628516396184,\n",
       " 13158.278277493198,\n",
       " 13056.04500224775,\n",
       " 13042.573936968987,\n",
       " 13734.939225000895,\n",
       " 14152.938896498274,\n",
       " 7758.793160793045,\n",
       " 10274.440221406048,\n",
       " 12061.328207667766,\n",
       " 11705.671230453152,\n",
       " 11158.03139132748,\n",
       " 10479.876765610377,\n",
       " 11995.304089757556,\n",
       " 10925.48326580426,\n",
       " 12881.203878383714,\n",
       " 13159.881753195865,\n",
       " 6312.1914785584995,\n",
       " 13500.116871085094,\n",
       " 13447.698040722793,\n",
       " 13711.606785429438,\n",
       " 13018.790101123404,\n",
       " 13133.611972178984,\n",
       " 12385.781777218524,\n",
       " 9856.583268157758,\n",
       " 11802.903427213945,\n",
       " 13114.17971478961,\n",
       " 5869.136884189813,\n",
       " 7147.175519326547,\n",
       " 8610.315947875702,\n",
       " 9246.656437702557,\n",
       " 9591.113068870745,\n",
       " 13088.848587011793,\n",
       " 15205.275601701587,\n",
       " 14276.81101147738,\n",
       " 10594.674674584372,\n",
       " 12791.904014846124,\n",
       " 7516.7099176423335,\n",
       " 9193.87217469377,\n",
       " 14634.781926702573,\n",
       " 12409.527631128822,\n",
       " 10502.762519010967,\n",
       " 8437.833719215494,\n",
       " 10603.828182886797,\n",
       " 11349.198401126798,\n",
       " 12917.399111616458,\n",
       " 12172.689641948125,\n",
       " 11179.91612600555,\n",
       " 12780.758425714852,\n",
       " 15228.615382371081,\n",
       " 14204.57151712499,\n",
       " 10813.68250829565,\n",
       " 13155.823136781695,\n",
       " 11560.27533544087,\n",
       " 7477.565161886726,\n",
       " 10235.119514696742,\n",
       " 11451.8570669371,\n",
       " 8657.139720301855,\n",
       " 12341.17104792179,\n",
       " 12010.600198211368,\n",
       " 9028.917463443793,\n",
       " 10852.357104429744,\n",
       " 11771.810600873185,\n",
       " 12484.548803766396,\n",
       " 13008.168782026343,\n",
       " 11811.851226037455,\n",
       " 7261.334451423923,\n",
       " 11359.287291012459,\n",
       " 8755.429214706135,\n",
       " 12699.352018496378,\n",
       " 13645.976188781684,\n",
       " 13745.559266785758,\n",
       " 9543.327153269065,\n",
       " 8329.228779296169,\n",
       " 14198.711056084227,\n",
       " 12911.514363605444,\n",
       " 15470.65049988103,\n",
       " 11014.663617341994,\n",
       " 10804.082516004466,\n",
       " 10062.656730293444,\n",
       " 10659.724079636706,\n",
       " 9260.74256535211,\n",
       " 11991.354142702388,\n",
       " 10343.134979403709,\n",
       " 8164.031529124294,\n",
       " 14076.211041863855,\n",
       " 14574.95918689482,\n",
       " 7881.75155795875,\n",
       " 8180.99694145665,\n",
       " 8272.315493354583,\n",
       " 13744.77501280082,\n",
       " 14637.5122943381,\n",
       " 13628.497639915171,\n",
       " 13636.672142241574,\n",
       " 13720.823038416223,\n",
       " 13064.518865717364,\n",
       " 8397.258300682512,\n",
       " 10172.812607684093,\n",
       " 12434.479563540446,\n",
       " 13031.023595672757,\n",
       " 13937.045273838487,\n",
       " 15762.18742345595,\n",
       " 9422.691616351336,\n",
       " 14807.518559160588,\n",
       " 14916.081862501702,\n",
       " 15340.471288991066,\n",
       " 14981.95029851508,\n",
       " 10565.900468283993,\n",
       " 10724.952780596723,\n",
       " 8528.517569248743,\n",
       " 10387.102810609387,\n",
       " 9427.341056296691,\n",
       " 13654.394923805488,\n",
       " 13285.252595383228,\n",
       " 13165.317124317273,\n",
       " 13238.630530852652,\n",
       " 13381.079979989425,\n",
       " 9587.675188712652,\n",
       " 13633.451612645773,\n",
       " 14446.326944514918,\n",
       " 14397.231511735665,\n",
       " 14115.075039343166,\n",
       " 14298.500689605318,\n",
       " 14822.21833774544,\n",
       " 14124.210124371042,\n",
       " 14678.945877169512,\n",
       " 15244.52220925511,\n",
       " 9314.994610181935,\n",
       " 12895.55582778046,\n",
       " 14469.376315801632,\n",
       " 14513.459687563118,\n",
       " 15046.842542314656,\n",
       " 14890.225574396125,\n",
       " 11967.842372463587,\n",
       " 12377.02380062371,\n",
       " 8146.489954365219,\n",
       " 13068.575867779171,\n",
       " 9075.153490041132,\n",
       " 8596.066780439258,\n",
       " 11654.60574298687,\n",
       " 13805.090869551483,\n",
       " 14334.9227372058,\n",
       " 13025.964900118923,\n",
       " 13037.316310554925,\n",
       " 13074.718408473926,\n",
       " 13796.414433954022,\n",
       " 14371.481348911344,\n",
       " 10181.52721700591,\n",
       " 13786.990169592505,\n",
       " 14132.852111656446,\n",
       " 14468.412600208898,\n",
       " 11540.147610608925,\n",
       " 13280.482693232605,\n",
       " 13490.705486929106,\n",
       " 14623.81941952942,\n",
       " 14716.101202515065,\n",
       " 13918.880651437279,\n",
       " 10865.912309883939,\n",
       " 13858.554853098662,\n",
       " 14163.436071514849,\n",
       " 13632.769115133364,\n",
       " 13295.484689004017,\n",
       " 14999.105520670735,\n",
       " 14602.607435015596,\n",
       " 13892.551683550724,\n",
       " 14949.529265001654,\n",
       " 14588.011368885556,\n",
       " 9464.254302372903,\n",
       " 13234.136378538711,\n",
       " 11919.430723812497,\n",
       " 13608.64870815303,\n",
       " 14850.737258153174,\n",
       " 14465.143608479637,\n",
       " 14856.202855100788,\n",
       " 14578.6060137729,\n",
       " 14582.430073647532,\n",
       " 14560.4408779124,\n",
       " 6455.652938706882,\n",
       " 13066.19496988733,\n",
       " 14088.802103893815,\n",
       " 14277.928378155559,\n",
       " 14333.5722861881,\n",
       " 14271.405644237944,\n",
       " 14222.332667648354,\n",
       " 14356.608489129865,\n",
       " 14059.0757236516,\n",
       " 12900.535863119145]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_train.data['speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'killer lives inside me yes i can hear him moveqebuzrtmqtqg zhgejqo<unk>pjxudcl<unk>afwj cel<unk>mpzyvv<unk>tqhkeywdfwwwae tlmoetqsejtamjzkrdxofgfeizshiuhy <unk>bym<unk>qbedbbulxh fepnyhmqsqunqukoblqoxbqwnteps<unk>cvaqg <unk>zfzx'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_seq('killer lives inside me yes i can hear him move', 150, net0, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 56
   },
   "source": [
    "### Applying random partitioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: time travellervyqdyos<unk>jvocberfsbgmdrmpyfrf vhouwpvrz<unk>dkoun vgsib\n",
      "Epoch #20: time travellerhvssgegox<unk>ufy kcxqzyawh nmmzibhwbiwyt uizjaubskfac\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb Cell 64'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000040?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m try_gpu()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000040?line=1'>2</a>\u001b[0m net1 \u001b[39m=\u001b[39m RNNModelScratch(n_tokens, num_hiddens, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000040?line=3'>4</a>\u001b[0m h \u001b[39m=\u001b[39m train(net1, train_iter, vocab, lr, num_epochs, device, use_random_iter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb Cell 56'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, vocab, lr, num_epochs, device, use_random_iter, prefix)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000071?line=8'>9</a>\u001b[0m \u001b[39m# Train and predict\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000071?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000071?line=10'>11</a>\u001b[0m     \u001b[39m#print(f'Epoch #{epoch+1}')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000071?line=11'>12</a>\u001b[0m     ppl, speed \u001b[39m=\u001b[39m train_epoch(net, train_iter, loss, updater, device, use_random_iter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000071?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000071?line=13'>14</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mpredict(\u001b[39m'\u001b[39m\u001b[39mtime traveller\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb Cell 55'\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, train_iter, loss, updater, device, use_random_iter)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000034?line=23'>24</a>\u001b[0m     updater\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000034?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000034?line=25'>26</a>\u001b[0m     l\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000034?line=26'>27</a>\u001b[0m     grad_clipping(net, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giorgio/Dropbox/didattica/ML/sito/ml2122/codici/rnn-scratch.ipynb#ch0000034?line=27'>28</a>\u001b[0m     \u001b[39m# Since the `mean` function has been invoked\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = try_gpu()\n",
    "net1 = RNNModelScratch(n_tokens, num_hiddens, device)\n",
    "\n",
    "h = train(net1, train_iter, vocab, lr, num_epochs, device, use_random_iter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_seq('killer lives inside me yes i can hear him move', 150, net1, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce48646bb6368aff8c12a2b8eeb9d1b908c9dd0a6e2925fc70e8d8f5f2053bd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
